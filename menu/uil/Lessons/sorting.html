<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
<title>Computer Science</title>
<BASE href="http://bwagner.dentonisd.org">
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<script language="JavaScript" src="scripts/menu.js"></script>
<script language="JavaScript" src="scripts/menu_items.js"></script>
<script language="JavaScript" src="scripts/menu_tpl.js"></script>
<link rel="stylesheet" href="stylesheets/menu.css">
<link rel="stylesheet" href="stylesheets/computerscience.css">
</head>
<body leftmargin="0" topmargin="0" marginwidth="0" marginheight="0">
<table width="100%" border="1" cellspacing="0" cellpadding="0">
  <tr bgcolor="#000066"> 
    <td> <div align="center"><font color="#FFFFFF"><strong>Sorting Algorithm</strong></font></div></td>
    <td> <div align="center"><font color="#FFFFFF"><strong>Implementation Summary</strong></font></div></td>
    <td> <div align="center"><font color="#FFFFFF"><strong>Comments</strong></font></div></td>
    <td> <div align="center"><font color="#FFFFFF"><strong>Type</strong></font></div></td>
    <td> <div align="center"><font color="#FFFFFF"><strong>Stable?</strong></font></div></td>
    <td> <div align="center"><font color="#FFFFFF"><strong>Big (O)</strong></font></div></td>
  </tr>
  <tr bgcolor="#D2E9FF"> 
    <td width="12%" bgcolor="#D2E9FF"><strong>Straight Insertion</strong></td>
    <td width="34%" bgcolor="#D2E9FF">On each pass the current item is inserted 
      into the sorted section of the list. It starts with the last position of 
      the sorted list, and moves backwards until it finds the proper place of 
      the current item. That item is then inserted into that place, and all items 
      after that are shuffled to the left to accommodate it. It is for this reason, 
      that if the list is already sorted, then the sorting would be O(n) because 
      every element is already in its sorted position. If however the list is 
      sorted in reverse, it would take O(n<sup>2</sup>) time as it would be searching through 
      the entire sorted section of the list each time it does an insertion, and 
      shuffling all other elements down the list.</td>
    <td width="29%">Good for nearly sorted lists, very bad for out of order lists, 
      due to the shuffling.</td>
    <td width="7%">Insertion<br> </td>
    <td width="5%">Yes </td>
    <td width="13%"> 
      <p align="left">Best Case: O(n)</p>
      <p>Worst Case: O(n<sup>2</sup>)</p></td>
  </tr>
  <tr bgcolor="#8AC5FF"> 
    <td><strong>Binary Insertion Sort</strong></td>
    <td>This is an extension of the Straight Insertion as above, however instead 
      of doing a linear search each time for the correct position, it does a binary 
      search, which is O(log n) instead of O(n). The only problem is that it always 
      has to do a binary search even if the item is in its current position. This 
      brings the cost of the best cast up to O(n log n). Due to the possibility 
      of having to shuffle all other elements down the list on each pass, the 
      worst case running time remains at O(n<sup>2</sup>). </td>
    <td>This is better than the Straight Insertion if the comparisons are costly. 
      This is because even though, it always has to do log n comparisons, it would 
      generally work out to be less than a linear search.</td>
    <td>Insertion</td>
    <td>Yes</td>
    <td> 
      <p align="left">Best Case: O(n log n).</p>
      <p>Worst Case: O(n<sup>2</sup>)</p></td>
  </tr>
  <tr bgcolor="#D2E9FF"> 
    <td><strong>Bubble Sort</strong></td>
    <td>On each pass of the data, adjacent elements are compared, and switched 
      if they are out of order. eg. e1 with e2, then e2 with e3 and so on. This 
      means that on each pass, the largest element that is left unsorted, has 
      been &quot;bubbled&quot; to its rightful place at the end of the array. 
      However, due to the fact that all adjacent out of order pairs are swapped, 
      the algorithm could be finished sooner. Preiss claims that it will always 
      take O(n<sup>2</sup>) time because it keeps sorting even if it is in order, as we can 
      see, the algorithm doesn't recognise that. Now someone with a bit more knowledge 
      than Preiss will obviously see, that you can end the algorithm in the case 
      when no swaps were made, thereby making the best case O(n) (when it is already 
      sorted) and worst case still at O(n<sup>2</sup>). </td>
    <td>In general this is better than Insertion Sort I believe, because it has 
      a good change of being sorted in much less than O(n<sup>2</sup>) time</td>
    <td>Exchange</td>
    <td>Yes</td>
    <td> 
      <p>Best Case: O(n).</p>
      <p>Worst Case: O(n<sup>2</sup>)</p></td>
  </tr>
  <tr bgcolor="#8AC5FF"> 
    <td><strong>Quicksort</strong></td>
    <td>The code is also useful and provided below (included is the selectPivot 
      method even though that probably won't help you understanding anyway).<br>
      The quick sort operates along these lines: Firstly a pivot is selected, 
      and removed from the list (hidden at the end). Then the elements are partitioned 
      into 2 sections. One which is less than the pivot, and one that is greater. 
      This partitioning is achieved by exchanging values. Then the pivot is restored 
      in the middle, and those 2 sections are recursively quick sorted.</td>
    <td bgcolor="#8AC5FF">A complicated but effective sorting algorithm.</td>
    <td>Exchange</td>
    <td>No</td>
    <td> 
      <p align="left">Best Case: O(n log n).</p>
      <p>Worst Case: O(n<sup>2</sup>)</p></td>
  </tr>
  <tr bgcolor="#D2E9FF"> 
    <td><strong>Selection Sort</strong></td>
    <td>This one, although not very efficient is very simply. Basically, it does 
      n<sup>2</sup> linear passes on the list, and on each pass, it selects the largest value, 
      and swaps it with the last unsorted element.<br>
      This means that it isn't stable, because for example a 3 could be swapped 
      with a 5 that is to the left of a different 3.</td>
    <td> 
      <p><br>
        A very simple algorithm, to code, and a very simple one to explain, but 
        a little slow.</p>
      <p>Note that you can do this using the smallest value, and swapping it with 
        the first unsorted element.</p></td>
    <td>Selection </td>
    <td>No</td>
    <td>Unlike the Bubble sort this one is truly O(n<sup>2</sup>), where best case and worst 
      case are the same, because even if the list is sorted, the same number of 
      selections must still be performed. </td>
  </tr>
  <tr bgcolor="#8AC5FF"> 
    <td><strong>Heap Sort</strong></td>
    <td>This uses a similar idea to the Straight Selection Sorting, except, instead 
      of using a linear search for the maximum, a heap is constructed, and the 
      maximum can easily be removed (and the heap reformed) in log n time. This 
      means that you will do n passes, each time doing a log n remove maximum, 
      meaning that the algorithm will always run in Q(n log n) time, as it makes 
      no difference the original order of the list. This utilises, just about 
      the only good use of heaps, that is finding the maximum element, in a max 
      heap (or the minimum of a min heap).</td>
    <td><br>
      Is in every way as good as the straight selection sort, but faster. </td>
    <td>Selection </td>
    <td>No</td>
    <td> 
      <p align="left">Best Case: O(n log n).</p>
      <p>Worst Case: O(n log n).</p>
      <p>Ok, now I know that looks tempting, but for a much more programmer friendly 
        solution, look at Merge sort instead, for a better O(n log n) sort.</p></td>
  </tr>
  <tr bgcolor="#D2E9FF"> 
    <td><strong>Merge Sort</strong></td>
    <td><br>
      It is fairly simple to take 2 sorted lists, and combine them into another 
      sorted list, simply by going through, comparing the heads of each list, 
      removing the smallest to join the new sorted list. As you may guess, this 
      is an O(n) operation. With 2 way sorting, we apply this method to an single 
      unsorted list. In brief, the algorithm recursively splits up the array until 
      it is fragmented into pairs of two single element arrays. Each of those 
      single elements is then merged with its pairs, and then those pairs are 
      merged with their pairs and so on, until the entire list is united in sorted 
      order. Noting that if there is every an odd number, an extra operation is 
      added, where it is added to one of the pairs, so that that particular pair 
      will have 1 more element than most of the others, and won't have any effect 
      on the actual sorting. </td>
    <td>Now isn't this much easier to understand that Heap sort, its really quite 
      intuitive. This one is best explain with the aid of the diagram, and if 
      you haven't already, you should look at it. </td>
    <td>Merge</td>
    <td>Yes</td>
    <td>Best and Worst Case: Q(n log n) </td>
  </tr>
  <tr bgcolor="#8AC5FF"> 
    <td><strong>Bucket Sort</strong></td>
    <td>Bucket sort initially creates a &quot;counts&quot; array whose size is 
      the size of the range of all possible values for the data we are sorting, 
      eg. all of the values could be between 1 and 100, therefore the array would 
      have 100 elements. 2 passes are then done on the list. The first tallies 
      up the occurrences of each of number into the &quot;counts&quot; array. 
      That is for each index of the array, the data that it contains signifies 
      the number of times that number occurred in list. The second and final pass 
      goes though the counts array, regenerating the list in sorted form. So if 
      there were 3 instance of 1, 0 of 2, and 1 of 3, the sorted list would be 
      recreated to 1,1,1,3. This diagram will most likely remove all shadows of 
      doubt in your minds. </td>
    <td>This sufferers a limitation that Radix doesn't, in that if the possible 
      range of your numbers is very high, you would need too many &quot;buckets&quot; 
      and it would be impractical. The other limitation that Radix doesn't have, 
      that this one does is that stability is not maintained. It does however 
      outperform radix sort if the possible range is very small. </td>
    <td>Distribution</td>
    <td>No</td>
    <td> 
      <p>No Best and Worst case:Q(m + n) where m is the number of possible values. 
        Obviously this is O(n) for most values of m, so long as m isn't too large.</p>
      <p>The reason that these distribution sorts break the O(n log n) barrier 
        is because no comparisons are performed!</p></td>
  </tr>
  <tr bgcolor="#D2E9FF"> 
    <td><strong>Radix Sort</strong></td>
    <td>This is an extremely spiffy implementation of the bucket sort algorithm. 
      This time, several bucket like sorts are performed (one for each digit), 
      but instead of having a counts array representing the range of all possible 
      values for the data, it represents all of the possible values for each individual 
      digit, which in decimal numbering is only 10. Firstly a bucked sort is performed, 
      using only the least significant digit to sort it by, then another is done 
      using the next least significant digit, until the end, when you have done 
      the number of bucket sorts equal to the maximum number of digits of your 
      biggest number. Because with the bucket sort, there are only 10 buckets 
      (the counts array is of size 10), this will always be an O(n) sorting algorithm! 
      See below for a Radix Example. On each of the adapted bucket sorts it does, 
      the count array stores the numbers of each digit. Then the offsets are created 
      using the counts, and then the sorted array regenerated using the offsets 
      and the original data. </td>
    <td>This is the god of sorting algorithms. It will search the largest list, 
      with the biggest numbers, and has a is guaranteed O(n) time complexity. 
      And it ain't very complex to understand or implement.<br>
      My recommendations are to use this one wherever possible.</td>
    <td>Distribution</td>
    <td>Yes</td>
    <td>Best and Worst Case: O(n)<br>
      Bloody awesome!</td>
  </tr>
</table>
<h2 align="left">Stable Sort</h2>
<p align="left">A <strong>stable</strong> sort is one where the initial order 
  of equal items is preserved. Some sorting algorithms are naturally stable, some 
  are unstable, and some can be made stable with care. Stability is important 
  when we want to sort by multiple fields--for example, sorting a list of task 
  assignments first by priority and then by assignee (in other words, assignments 
  of equal priority are sorted by assignee). One easy way to do this is to sort 
  by assignee first, then take the resulting sorted list and sort that by priority. 
  This only works if the sorting algorithm is stable--otherwise, the sortedness-by-assignee 
  of equal-priority items is not preserved. </p>
<p align="left">For example, if we sort the words &quot;apple&quot;, &quot;tree&quot; 
  and &quot;pink&quot; by length, then &quot;tree&quot;, &quot;pink&quot;, &quot;apple&quot; 
  is a stable sort but &quot;pink&quot;, &quot;tree&quot;, apple&quot; is not. 
</p>
<p align="left"><strong>MergeSort</strong> is a very common choice of stable sorts, 
  achieved by favoring the leftmost item in each merge step.<br>
  <strong>RadixSort</strong> is another of the stable sorting algorithms. </p>
<p align="left">&nbsp;</p>
<h2 align="left">Radix Sort Example</h2>
<p><strong>First Pass:</strong></p>
<p><strong>Data:</strong> 67 50 70 25 93 47 21</p>
<p><strong>Buckets on first pass (least significant digit):</strong><br>
  index 0 1 2 3 4 5 6 7 8 9<br>
  count 2 1 0 1 0 1 0 2 0 0<br>
  offset 0 2 3 3 4 4 5 5 7 7</p>
<p><br>
  <strong>Data after first pass</strong><br>
  50 70 21 93 25 67 47</p>
<p>That data is created by doing a single pass on the unsorted data, using the 
  offsets to work out at where each item belongs.<br>
  For example, it looks at the first one 67, then at the offsets for the digit 
  7, and inserts it into the 5th position. The offset at 7 is then incremented, 
  so that the next value encountered which has a least significant digit of 7 
  is placed into the 6th position. Continuing the example, the number 50 would 
  then be looked at, and inserted into the 0th position, its offset incremented, 
  so that the next value which is 70 would be inserted into the 1st position, 
  and so on until then end of the list.<br>
  As you can see, this data is sorted by its least significant digit.</p>
<p><strong>Second Pass:</strong></p>
<p><strong>Data after first pass</strong><br>
  50 70 21 93 25 67 47</p>
<p><strong>Buckets on first pass (most significant digit):</strong><br>
  index 0 1 2 3 4 5 6 7 8 9<br>
  count 0 0 2 0 1 1 1 1 0 1<br>
  offset 0 2 3 3 4 4 5 5 7 7 </p>
<p><strong>Data after second pass (sorted)</strong><br>
  21 25 47 50 67 70 93</p>
<p></p>
<h1 align="left">&nbsp; </h1>
